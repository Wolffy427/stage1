{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ff8e8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3-8B-Instruct/original/tokenizer.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      8\u001b[39m tokenizer_path = \u001b[33m\"\u001b[39m\u001b[33m~/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3-8B-Instruct/original/tokenizer.model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m special_tokens = [\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m<|begin_of_text|>\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m<|end_of_text|>\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m<|eot_id|>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# end of turn\u001b[39;00m\n\u001b[32m     20\u001b[39m ] + [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<|reserved_special_token_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m, \u001b[32m256\u001b[39m - \u001b[32m5\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m mergeable_ranks = \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m tokenizer = tiktoken.Encoding(\n\u001b[32m     23\u001b[39m     name=Path(tokenizer_path).name,\n\u001b[32m     24\u001b[39m     pat_str=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(?i:\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm|\u001b[39m\u001b[33m'\u001b[39m\u001b[33mll|\u001b[39m\u001b[33m'\u001b[39m\u001b[33md)|[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[33m\\\u001b[39m\u001b[33mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[33m]?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[33m+|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[33m{\u001b[39m\u001b[33m1,3}| ?[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[33m\\\u001b[39m\u001b[33mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[33m]+[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn]*|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn]+|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+(?!\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS)|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     mergeable_ranks=mergeable_ranks,\n\u001b[32m     26\u001b[39m     special_tokens={token: \u001b[38;5;28mlen\u001b[39m(mergeable_ranks) + i \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(special_tokens)}\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m tokenizer.decode(tokenizer.encode(\u001b[33m\"\u001b[39m\u001b[33mhello world!\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project/stage1/stage1/lib/python3.13/site-packages/tiktoken/load.py:148\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     contents = \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     ret = {}\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m contents.splitlines():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project/stage1/stage1/lib/python3.13/site-packages/tiktoken/load.py:63\u001b[39m, in \u001b[36mread_file_cached\u001b[39m\u001b[34m(blobpath, expected_hash)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m contents = \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m expected_hash \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_hash(contents, expected_hash):\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     66\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHash mismatch for data downloaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblobpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis may indicate a corrupted download. Please try again.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project/stage1/stage1/lib/python3.13/site-packages/tiktoken/load.py:16\u001b[39m, in \u001b[36mread_file\u001b[39m\u001b[34m(blobpath)\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     14\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mblobfile is not installed. Please install it by running `pip install blobfile`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mblobfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBlobFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     17\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# avoiding blobfile for public files helps avoid auth issues, like MFA prompts\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project/stage1/stage1/lib/python3.13/site-packages/blobfile/_ops.py:393\u001b[39m, in \u001b[36mBlobFile\u001b[39m\u001b[34m(path, mode, streaming, buffer_size, cache_dir, file_size, version)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mBlobFile\u001b[39m(\n\u001b[32m    364\u001b[39m     path: RemoteOrLocalPath,\n\u001b[32m    365\u001b[39m     mode: Literal[\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mab\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    370\u001b[39m     version: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    371\u001b[39m ):\n\u001b[32m    372\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    Open a local or remote file for reading or writing\u001b[39;00m\n\u001b[32m    374\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m \u001b[33;03m        A file-like object\u001b[39;00m\n\u001b[32m    392\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBlobFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project/stage1/stage1/lib/python3.13/site-packages/blobfile/_context.py:894\u001b[39m, in \u001b[36mContext.BlobFile\u001b[39m\u001b[34m(self, path, mode, streaming, buffer_size, cache_dir, file_size, version)\u001b[39m\n\u001b[32m    892\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[33m\"\u001b[39m\u001b[33mCannot specify cache_dir for streaming files\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_local_path(path):\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     f = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFileIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    896\u001b[39m         f = io.BufferedReader(f, buffer_size=buffer_size)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '~/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3-8B-Instruct/original/tokenizer.model'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer_path = \"~/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3-8B-Instruct/original/tokenizer.model\"\n",
    "special_tokens = [\n",
    "    \"<|begin_of_text|>\",\n",
    "    \"<|end_of_text|>\",\n",
    "    \"<|reserved_special_token_0|>\",\n",
    "    \"<|reserved_special_token_1|>\",\n",
    "    \"<|reserved_special_token_2|>\",\n",
    "    \"<|reserved_special_token_3|>\",\n",
    "    \"<|start_header_id|>\",\n",
    "    \"<|end_header_id|>\",\n",
    "    \"<|reserved_special_token_4|>\",\n",
    "    \"<|eot_id|>\", # end of turn\n",
    "] + [f\"<|reserved_special_token_{i}\" for i in range(5, 256 - 5)]\n",
    "mergeable_ranks = load_tiktoken_bpe(tokenizer_path)\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=Path(tokenizer_path).name,\n",
    "    pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)}\n",
    ")\n",
    "tokenizer.decode(tokenizer.encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef432b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
