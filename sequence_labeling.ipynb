{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c129d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "categories = set()\n",
    "\n",
    "class PeopleDaily(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f.read().split('\\n\\n')):\n",
    "                if not line:\n",
    "                    break\n",
    "                sentence, labels = '', []\n",
    "                for i, item in enumerate(line.split('\\n')):\n",
    "                    char, tag = item.split(' ')\n",
    "                    sentence += char\n",
    "                    if tag.startswith('B'):\n",
    "                        labels.append([i, i, char, tag[2:]])\n",
    "                        categories.add(tag[2:])\n",
    "                    elif tag.startswith('I'):\n",
    "                        labels[-1][1] = i\n",
    "                        labels[-1][2] += char\n",
    "                Data[idx] = {\n",
    "                    'sentence': sentence,\n",
    "                    'labels': labels\n",
    "                }\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd5549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': '海钓比赛地点在厦门与金门之间的海域。', 'labels': [[7, 8, '厦门', 'LOC'], [10, 11, '金门', 'LOC']]}\n"
     ]
    }
   ],
   "source": [
    "train_data = PeopleDaily('data/china-people-daily-ner-corpus/example.train')\n",
    "valid_data = PeopleDaily('data/china-people-daily-ner-corpus/example.dev')\n",
    "test_data = PeopleDaily('data/china-people-daily-ner-corpus/example.test')\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42ad4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: 'B-LOC', 2: 'I-LOC', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-PER', 6: 'I-PER'}\n",
      "{'0': 0, 'B-LOC': 1, 'I-LOC': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-PER': 5, 'I-PER': 6}\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"0\"}\n",
    "for c in list(sorted(categories)):\n",
    "    id2label[len(id2label)] = f\"B-{c}\"\n",
    "    id2label[len(id2label)] = f\"I-{c}\"\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20107923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouke/Documents/project/stage1/stage1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。', '[SEP]']\n",
      "[0 0 0 0 0 0 0 0 1 2 0 1 2 0 0 0 0 0 0 0]\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', 'B-LOC', 'I-LOC', '0', 'B-LOC', 'I-LOC', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentence = \"海钓比赛地点在厦门与金门之间的海域。\"\n",
    "labels = [[7, 8, '厦门', 'LOC'], [10, 11, '金门', 'LOC']]\n",
    "\n",
    "encoding = tokenizer(sentence, truncation=True)\n",
    "tokens = encoding.tokens()\n",
    "label = np.zeros(len(tokens), dtype=int)\n",
    "for char_start, char_end, word, tag in labels:\n",
    "    token_start = encoding.char_to_token(char_start)\n",
    "    token_end = encoding.char_to_token(char_end)\n",
    "    label[token_start] = label2id[f\"B-{tag}\"]\n",
    "    label[token_start + 1: token_end + 1] = label2id[f\"I-{tag}\"]\n",
    "\n",
    "print(tokens)\n",
    "print(label)\n",
    "print([id2label[id] for id in label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aff90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 39]), 'token_type_ids': torch.Size([4, 39]), 'attention_mask': torch.Size([4, 39])}\n",
      "batch_y shape: torch.Size([4, 39])\n",
      "{'input_ids': tensor([[ 101, 1762, 1344, 1999,  510, 1344, 3124, 2424, 7566, 2193, 5468, 2375,\n",
      "          833,  677, 8024, 7440, 6958, 1828, 2990, 1139,  100, 1059, 1344,  782,\n",
      "         1772,  671,  774, 3365, 3409,  100, 4638, 6226, 1153,  511,  102,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 4617, 4568, 2434, 1908, 5442,  812, 4638, 5736, 7410, 1325, 4923,\n",
      "         8024, 2245, 4850, 1139, 8025, 8025, 8025, 8020,  683, 7579, 2845, 6887,\n",
      "         8021, 8020, 7353, 1745, 4275,  122, 2476, 8021,  102,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 1912, 4545, 8038, 3221, 6825, 5330,  124,  702, 3299, 1762, 4649,\n",
      "         2938, 6956,  855, 3864, 3300, 1068, 4638, 1912, 4500, 5790, 4289,  511,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 1952, 2209, 2357, 6609, 4294, 1762,  928,  704, 4917, 8024, 6564,\n",
      "         2209, 3419, 5812, 2548, 2553, 7557,  977, 3632, 6818, 1126, 1453, 6822,\n",
      "         6121, 4638,  100, 1314, 1350, 2190, 6413, 4638, 3274, 1213, 6121, 1220,\n",
      "          100,  511,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[-100,    0,    3,    4,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    5,    6,    6,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
      "         -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100,\n",
      "         -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100],\n",
      "        [-100,    5,    6,    6,    6,    6,    0,    0,    0,    0,    0,    1,\n",
      "            2,    2,    2,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, -100]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence, batch_tags = [], []\n",
    "    for sampele in batch_samples:\n",
    "        batch_sentence.append(sampele['sentence'])\n",
    "        batch_tags.append(sampele['labels'])\n",
    "    batch_inputs = tokenizer(\n",
    "        batch_sentence,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    batch_label = np.zeros(batch_inputs['input_ids'].shape, dtype=int)\n",
    "    for s_idx, sentence in enumerate(batch_sentence):\n",
    "        encoding = tokenizer(sentence, truncation=True)\n",
    "        batch_label[s_idx][0] = -100\n",
    "        batch_label[s_idx][len(encoding.tokens()) - 1:] = -100\n",
    "        for char_start, char_end, _, tag in batch_tags[s_idx]:\n",
    "            token_start = encoding.char_to_token(char_start)\n",
    "            token_end = encoding.char_to_token(char_end)\n",
    "            batch_label[s_idx][token_start] = label2id[f\"B-{tag}\"]\n",
    "            batch_label[s_idx][token_start + 1: token_end + 1] = label2id[f\"I-{tag}\"]\n",
    "    return batch_inputs, torch.tensor(batch_label)\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cf1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744d45aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForNER were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForNER(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class BertForNER(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(768, len(id2label))\n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert(**x)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForNER.from_pretrained(checkpoint, config=config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df9096a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 39, 7])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(batch_X)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48e2d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(data_loader, model, loss_fn, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(data_loader)))\n",
    "    progress_bar.set_description(f\"loss: {0:>7f}\")\n",
    "    finish_batch_num = (epoch - 1) * len(data_loader)\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(data_loader, start=1):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred.permute(0, 2, 1), y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f\"loss: {total_loss / (finish_batch_num + batch):7f}\")\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b384af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.50      0.50      0.50         2\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.75      0.75      0.75         3\n",
      "weighted avg       0.67      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "y_true = [['O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'B-LOC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "y_pred = [['O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'B-LOC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode='strict', scheme=IOB2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e55a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "def test_loop(dadaloader, model):\n",
    "    true_labels, true_predictions = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(dadaloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            predictions = pred.argmax(dim=-1).cpu().numpy().tolist()\n",
    "            labels = y.cpu().numpy().tolist()\n",
    "            true_labels += [[id2label[int(l)] for l in label if l != -100] for label in labels]\n",
    "            true_predictions += [\n",
    "                [id2label[int(p)] for (p, l) in zip(predictions, label) if l != 100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "    print(classification_report(true_labels, true_predictions, mode='strict', scheme=IOB2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ae736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.183897: 100%|██████████| 5216/5216 [25:02<00:00,  3.47it/s]\n",
      "  0%|          | 0/580 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     total_loss = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, t + \u001b[32m1\u001b[39m, total_loss)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtest_loop\u001b[39m\u001b[34m(dadaloader, model)\u001b[39m\n\u001b[32m     13\u001b[39m         labels = y.cpu().numpy().tolist()\n\u001b[32m     14\u001b[39m         true_labels += [[id2label[\u001b[38;5;28mint\u001b[39m(l)] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m label \u001b[38;5;28;01mif\u001b[39;00m l != -\u001b[32m100\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[32m     15\u001b[39m         true_predictions += [\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m             [id2label[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, label) \u001b[38;5;28;01mif\u001b[39;00m l != \u001b[32m100\u001b[39m]\n\u001b[32m     17\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[32m     18\u001b[39m         ]\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(true_labels, true_predictions, mode=\u001b[33m'\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m'\u001b[39m, scheme=IOB2))\n",
      "\u001b[31mTypeError\u001b[39m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "lr = 1e-5\n",
    "epoch_num = 1\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=0,\n",
    "    num_warmup_steps=epoch_num * len(train_dataloader)\n",
    ")\n",
    "\n",
    "total_loss = 0\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t + 1} / {epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, t + 1, total_loss)\n",
    "    test_loop(valid_dataloader, model)\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
