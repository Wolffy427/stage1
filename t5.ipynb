{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730fe8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "max_dataset_size = 20000\n",
    "\n",
    "class DRQG(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx >= max_dataset_size:\n",
    "                    break\n",
    "                data = json.loads(line)\n",
    "                Data[idx] = {\n",
    "                    'question': data['question'],\n",
    "                    'context': data['context'],\n",
    "                    'answer': data['answer'],\n",
    "                    'id': data['id']\n",
    "                }\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_data = DRQG('data/DuReaderQG/train.json')\n",
    "valid_data = DRQG('data/DuReaderQG/dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ae6931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 14520\n",
      "valid set size: 984\n",
      "{'question': '仙剑奇侠传3第几集上天界', 'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。', 'answer': '第35集', 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de45ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouke/Documents/project/stage1/stage1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'langboat/mengzi-t5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387fcf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2900, 538, 232, 105, 3, 8, 4574, 278, 191, 2074, 4, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['▁我', '叫', '张', '三', ',', '在', '苏州', '大学', '学习', '计算机', '。', '</s>']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"我叫张三，在苏州大学学习计算机。\")\n",
    "print(inputs)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs.input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2a61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    7,  1707,  1467,  ...,  6453,     4,     1],\n",
      "        [    7,  9147, 14702,  ...,     0,     0,     0],\n",
      "        [    7, 19918,   176,  ...,     0,     0,     0],\n",
      "        [    7,  2747,  4403,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "context = [train_data[idx]['context'] for idx in range(4)]\n",
    "question = [train_data[idx]['question'] for idx in range(4)]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=512,\n",
    "    truncation='only_second',\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4938927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '仙', '剑', '奇', '侠', '传', '3', '第', '几', '集', '上天', '界', '</s>', '▁第', '35', '集', '雪', '见', '缓缓', '张开', '眼睛', ',', '景', '天', '又', '惊', '又', '喜', '之际', ',', '长', '卿', '和', '紫', '萱', '的', '仙', '船', '驶', '至', ',', '见', '众人', '无', '恙', ',', '也十分', '高兴', '。', '众人', '登', '船', ',', '用', '尽', '合力', '把', '自身的', '真', '气', '和', '水分', '输给', '她', '。', '雪', '见', '终于', '醒', '过来', '了', ',', '但却', '一脸', '木', '然', ',', '全', '无', '反应', '。', '众人', '向', '常', '胤', '求助', ',', '却发现', '人', '世界', '竟', '没有', '雪', '见', '的', '身世', '纪录', '。', '长', '卿', '询问', '清', '微', '的', '身世', ',', '清', '微', '语', '带', '双', '关', '说', '一切', '上了', '天', '界', '便', '有', '答案', '。', '长', '卿', '驾驶', '仙', '船', ',', '众人', '决定', '立马', '动', '身', ',', '往', '天', '界', '而去', '。', '众人', '来到', '一', '荒', '山', ',', '长', '卿', '指出', ',', '魔', '界', '和', '天', '界', '相连', '。', '由', '魔', '界', '进入', '通过', '神', '魔', '之', '井', ',', '便可', '登', '天', '。', '众人', '至', '魔', '界', '入口', ',', '仿', '若', '一', '黑色的', '蝙蝠', '洞', ',', '但', '始终', '无法', '进入', '。', '后来', '花', '楹', '发现', '只要有', '翅膀', '便', '能', '飞', '入', '。', '于是', '景', '天', '等人', '打下', '许多', '乌鸦', ',', '模仿', '重', '楼', '的', '翅膀', ',', '制作', '数', '对', '翅膀', '状', '巨', '物', '。', '刚', '佩戴', '在', '身', ',', '便', '被', '吸入', '洞', '口', '。', '众人', '摔', '落在', '地', ',', '抬头', '发现', '魔', '界', '守卫', '。', '景', '天', '和', '众', '魔', '套', '交', '情', ',', '自称', '和', '魔', '尊重', '楼', '相', '熟', ',', '众', '魔', '不理', ',', '打', '了起来', '。', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs.input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b345c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "max_input_length = 512\n",
    "max_target_length = 256\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5976315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch_samples):\n",
    "    batch_question, batch_context = [], []\n",
    "    batch_target = []\n",
    "    for sample in batch_samples:\n",
    "        batch_question.append(sample['question'])\n",
    "        batch_context.append(sample['context'])\n",
    "        batch_target.append(sample['answer'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_question,\n",
    "        batch_context,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation='only_second',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_target,\n",
    "            max_length=max_target_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )['input_ids']\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_idx = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_idx):\n",
    "            labels[idx][end_idx + 1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4128d06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'])\n",
      "batch shape: {'input_ids': torch.Size([4, 207]), 'attention_mask': torch.Size([4, 207]), 'decoder_input_ids': torch.Size([4, 13]), 'labels': torch.Size([4, 13])}\n",
      "{'input_ids': tensor([[    7,  8613, 18784, 10912,  5349,     1,     7,   838,    68,  2946,\n",
      "            90,     3,   354, 10526,   125,    43,  4057,  7698, 25018,     3,\n",
      "          6318,  3162,  8210, 18784, 12611, 10047,     3,   354,   883,     5,\n",
      "             3,   112,   851,    10,  2324,  7999,    10,  4734,  8613, 18784,\n",
      "         10526,  5349,    66,  7698, 25018,  1449,     4, 11307,   600,  1619,\n",
      "           366,   993,   495,   177,   364,  4932,  7372,    74,   364,  4932,\n",
      "          7372,  8395,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [    7,  1548,   174,   259,  6430,  7001,     1,  3621, 27561,   323,\n",
      "          2014,     9,   731,  1548,  4371, 24747,  5363,   276,     3,   461,\n",
      "           174,   259,     5,   775,   162,  7001,   153,  8319,  5162,    94,\n",
      "           394,  7484,  5162,    22,   549,    50,    25,   138,   394,     4,\n",
      "          4734,    99, 27561,  1453,    27,   231,  1548,  3081,  5363,   259,\n",
      "             3,  2071,  4371,   438,  2218,   775,   162,  7101,     3, 15000,\n",
      "          4032,  2218,   775,   162,  7101,   418,   124,  3921,     5,   775,\n",
      "           162,     4,   199,  9222,   201, 27561,  7530,  5363,   276,    52,\n",
      "           676, 19310,  8959,   524,  3186,     4,     1,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [    7,  4703,     7,  4357,     1,     7, 24930,  2166,  4703,  3093,\n",
      "         19104,  2144, 12735,   813,  4894, 15425,   321,  3383,   384,   351,\n",
      "          9394,     3,   773,   444,  1920,   351, 12311,   384,   184,  1202,\n",
      "             3,  9394,  4357,   247,    18,   218,  4989,   173,  4500,  9181,\n",
      "           108,  4399,  1073,  4500,  9181, 26008,  4500,     4,  2357,   243,\n",
      "           622,    19,  1416,   622,  2935,   541,     4,  4734,  9394,  4777,\n",
      "            22,  2855,   245,    25,   247,    15, 25132,  2463,     9,  1169,\n",
      "          4996,   611,  2192,   235,     3,    91,    15,   198,  8954,     6,\n",
      "          9394,  2855,     6,  5959,  6278,    86,  9048, 16088,     4,  9394,\n",
      "         12611,   247,    15,  6530,     6,   839,   489,     6,  1632,     6,\n",
      "          2985,  4621,     6,   711,   640,  7208,  7713,   640,  8359,     6,\n",
      "           471,  1171,  4703,  7786,     9,  1758,   396,   494,   219,    92,\n",
      "          2192,   235,     3,  9394,  1860,    49,  1687,   229,  1984,    18,\n",
      "          2469,  5192,   229,  2469, 13355,   121,   229,  2469,  5192,     3,\n",
      "          2655,  5155, 24922,  1720,  7018,  7635,     4,     1,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [    7,  2074,    16,     3,    94,  1529,   938,  6196,     1,     7,\n",
      "          2340,  2125,    13,  3621,  1529,   920,  3479,    94,  1529,   920,\n",
      "          1701,  7368,   875,  3479,   165,  1367,   920,  1701,  7368,   875,\n",
      "             7,   782,  6334,    13,     7,  1529,   920,    66,  1529,   920,\n",
      "          1701,  7368,   875, 11641,  1701,  7368,   875,    66,    31,   488,\n",
      "           635,    41,  4512,    30,     7,  2092,    66,  1070,  4512,     3,\n",
      "            93,  2074,  8916,   195,   337,   568,    48,  1394,     3,   165,\n",
      "            92,    94,  4386,    11,   165,  1367,   506,   506,   506,   506,\n",
      "           506,    22,   195,   337,   568,    25,  3479,   165,  1367,    22,\n",
      "           750,   337,   568,    25,    94,   920,  1701,  7368,   875,    22,\n",
      "           488,   635,    25,  3479,   218,   938,  5366,    22,   151,   383,\n",
      "            25,     7,    55,  3591,  8763,    74,    55, 11320,  1823,  5792,\n",
      "         21274,     3,  8648, 14548,   635,     7,    55, 10291,    66,   611,\n",
      "         21274,     3,  8648, 14246,   635,     4,     7,   354,   531,  2671,\n",
      "          9036,   920,  1701,  7368,   875,     3,    28,  4057,    31,   920,\n",
      "           661,   531,   442, 19441,  9036,   938,  5366,     3,  1029,    11,\n",
      "            43,  4057,    31,   938,   740,     7,  2505,    13,    55,  1560,\n",
      "            15,   218, 24389,  1701,  7368,   875,     7,  2505,    13,    14,\n",
      "          3445,  1560,     5, 19441,    11,  1336,   173,  2092,   920,   742,\n",
      "           875,     3,   397,    99,  1709,   938,  5366,     3,    62,  1893,\n",
      "         11868, 13108,  5400,     5, 19441,     4,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'decoder_input_ids': tensor([[    0,     7,  7698, 25018,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    0,  7973,   394,     1,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    0, 14669,  4989,   173,  4500,  9181,   108,  4399,  1073,  4500,\n",
      "          9181, 26008,  4500],\n",
      "        [    0, 13213,  1367,   920,  1701,  7368,   875,     1,     0,     0,\n",
      "             0,     0,     0]]), 'labels': tensor([[    7,  7698, 25018,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [ 7973,   394,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [14669,  4989,   173,  4500,  9181,   108,  4399,  1073,  4500,  9181,\n",
      "         26008,  4500,     1],\n",
      "        [13213,  1367,   920,  1701,  7368,   875,     1,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouke/Documents/project/stage1/stage1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b290a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch - 1) * len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss / (finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4139b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge [{'rouge-1': {'r': 0.75, 'p': 0.8, 'f': 0.7741935433922998}, 'rouge-2': {'r': 0.5625, 'p': 0.5625, 'f': 0.562499995}, 'rouge-l': {'r': 0.6875, 'p': 0.7333333333333333, 'f': 0.7096774143600416}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "generated_summary = \"我在苏州大学学习计算机，苏州大学很美丽。\"\n",
    "reference_summary = \"我在环境优美的苏州大学学习计算机。\"\n",
    "\n",
    "TOKENIZE_CHINESE = lambda x: ' '.join(x)\n",
    "\n",
    "scores = rouge.get_scores(\n",
    "    hyps=[TOKENIZE_CHINESE(generated_summary)],\n",
    "    refs=[TOKENIZE_CHINESE(reference_summary)]\n",
    ")\n",
    "print('Rouge', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d7b9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    preds, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data['input_ids'],\n",
    "                attention_mask=batch_data['attention_mask'],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=2\n",
    "            ).cpu().numpy()\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data['labels'].cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [' '.join(pred.strip()) for pred in decoded_preds]\n",
    "        labels += [' '.join(label.strip()) for label in decoded_labels]\n",
    "    \n",
    "    scores = rouge.get_scores(hyps=preds, refs=labels, avg=True)\n",
    "    result = {key: value['f'] * 100 for key, value in scores.items()}\n",
    "    print(f\"Rouge1: {result['rouge-1']:>0.2f} Rouge2: {result['rouge-2']:>0.2f} RougeL: {result['rouge-l']:>0.2f}\\n\")\n",
    "    return  result\n",
    "    # return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = test_loop(valid_dataloader, model)\n",
    "preds, labels = test_loop(valid_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForSeq2SeqLM\n",
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc48ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = iter(train_dataloader)\n",
    "for _ in range(4):\n",
    "    batch_data = next(valid_iter)\n",
    "    print(batch_data)\n",
    "    generated = model1.generate(\n",
    "        batch_data['input_ids'],\n",
    "        attention_mask=batch_data['attention_mask'],\n",
    "        max_length=max_target_length,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    print('generated', generated)\n",
    "    decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rouge.get_scores(hyps=preds, refs=labels, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9db229",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (pred, label) in enumerate(zip(preds, labels)):\n",
    "    if not label or not pred:\n",
    "        print('pred: ', pred)\n",
    "        print('label:', label)\n",
    "        print('idx:', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437d1e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "loss: 0.000000:   0%|          | 0/3630 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "loss: 7.795273:   0%|          | 6/3630 [00:14<1:52:23,  1.86s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     total_loss = \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     valid_rouge = test_loop(valid_dataloader, model)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(valid_rouge)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(dataloader, model, optimizer, lr_scheduler, epoch, total_loss)\u001b[39m\n\u001b[32m     12\u001b[39m loss = outputs.loss\n\u001b[32m     14\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m optimizer.step()\n\u001b[32m     17\u001b[39m lr_scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project/stage1/stage1/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project/stage1/stage1/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project/stage1/stage1/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 1\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_avg_rouge = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_rouge = test_loop(valid_dataloader, model)\n",
    "    print(valid_rouge)\n",
    "    rouge_avg = valid_rouge['avg']\n",
    "    if rouge_avg > best_avg_rouge:\n",
    "        best_avg_rouge = rouge_avg\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_rouge_{rouge_avg:0.4f}_model_weights.bin')\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0f87c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
