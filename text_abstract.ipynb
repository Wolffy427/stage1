{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6fccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "max_dataset_size = 2000\n",
    "\n",
    "class LCSTS(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx >= max_dataset_size:\n",
    "                    break\n",
    "                items = line.strip().split(\"!=!\")\n",
    "                assert len(items) == 2\n",
    "                Data[idx] = {\n",
    "                    'title': items[0],\n",
    "                    'content': items[1]\n",
    "                }\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "train_data = LCSTS('data/lcsts_tsv/data1.tsv')\n",
    "valid_data = LCSTS('data/lcsts_tsv/data2.tsv')\n",
    "test_data = LCSTS('data/lcsts_tsv/data3.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dadfd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 2000\n",
      "valid set size: 2000\n",
      "test set size: 1106\n",
      "{'title': '修改后的立法法全文公布', 'content': '新华社受权于18日全文播发修改后的《中华人民共和国立法法》，修改后的立法法分为“总则”“法律”“行政法规”“地方性法规、自治条例和单行条例、规章”“适用与备案审查”“附则”等6章，共计105条。'}\n"
     ]
    }
   ],
   "source": [
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7a0d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouke/Documents/project/stage1/stage1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/zhouke/Documents/project/stage1/stage1/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4517f948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [259, 165389, 117707, 9792, 6696, 123553, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "['▁', '我在', '苏州', '大学', '学', '计算机', '</s>']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"我在苏州大学学计算机\")\n",
    "print(inputs)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs.input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df49c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['content'])\n",
    "        batch_targets.append(sample['title'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets,\n",
    "            padding=True,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )['input_ids']\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_index in enumerate(end_token_index):\n",
    "            labels[idx][end_index + 1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False, collate_fn=collote_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee63964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'])\n",
      "batch shape: {'input_ids': torch.Size([4, 72]), 'attention_mask': torch.Size([4, 72]), 'decoder_input_ids': torch.Size([4, 17]), 'labels': torch.Size([4, 17])}\n",
      "{'input_ids': tensor([[   259, 109505, 120788,   4002,    591,  99441, 200719,   3541,    261,\n",
      "          10389,  33420,  19115,  24705,    365,   2037,  74090,    261,  15268,\n",
      "         188757,  71650, 126455, 133373,  16436,  31761,    292,  14428,   5718,\n",
      "         122670,   7973, 195050,    292,  97540,  36385,    292, 124916,   5811,\n",
      "          20760,  14735,    292, 135926, 139877,    292,  31651,  37488,    292,\n",
      "          11764,  62369,   1107, 125876,  56203,   2395,    449,   1146,  17723,\n",
      "            306,  21239, 122153,  13061,  28169,   8773,   2811,  15268,   4775,\n",
      "           6642,    306,      1,      0,      0,      0,      0,      0,      0],\n",
      "        [   809,    848,  12267, 195305,  32063,  46601, 172901,  71813,  36633,\n",
      "          46601,   7087,  80797,   9903,  37940,   8637,    306,  21410,    328,\n",
      "            891,    353, 136848,    261,  49535, 167507,  25955,  46601,  10306,\n",
      "           6589,   1597,   3862,   2395,    261,   3801,  22136, 149063,   5638,\n",
      "         111848,  13946,  11522,    306,  59029, 171729,  62784, 164358,  50156,\n",
      "         172901,  33511,  36633,  46601,  94307,   2640, 219454, 205080,  19783,\n",
      "           4484,   6040,  23827, 164411,  35525,  25470,  15268,    261,  72600,\n",
      "          31773,  32063,  46601,   7715,   3661,   4484,   4066,    306,      1],\n",
      "        [   809, 219570,    312,  76581,    271,   1854,   6450,  12531, 210972,\n",
      "         179204,  35798,   2219, 158040,   1087,    891,   1087, 143173,    495,\n",
      "           2688,   1083,  76581,   1854,   2219,  14210,  11834, 167042,    306,\n",
      "         109505, 123290,  10139,   8385,   1107,  23464,   2219,  23464,  14210,\n",
      "           8385,  13061,  20313, 179750,    261,  68397,  22331,  95364,  14210,\n",
      "           5396,    306,  81318,   2219,  10306,   3661,  75013,   4425,  69957,\n",
      "            261,   5028,  46259,    591,  27854,  71922,   1206,  86998,  43183,\n",
      "            292, 134207,   7154,  21158,   4462,    493,  15177,    306,      1],\n",
      "        [   259,   8813,  41297,   6753,  23827,  76581,  13524,  36070,   3586,\n",
      "         100289, 156695,   5674,    291,   8813,  44223,   6753,    848,  55548,\n",
      "            660,   3203,  41755,  24749, 223632,   5507, 143203,   5674,    291,\n",
      "           8813,   4833, 116108, 114526, 192395, 105418,    493,  96280,  28251,\n",
      "          27378,  21158,   5674,    291,   2372,    891,   1653, 190786,  59001,\n",
      "           2811, 112765,    261, 142017, 148428,   3586, 100289,  96280,  28251,\n",
      "          27378,  21158, 131011, 200127,  46810,    261,   2251,   8922,   4484,\n",
      "          10306,  51385,   4425,   7154,   4235,    309, 239852,      1,      0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'decoder_input_ids': tensor([[     0,    809,  76581,  10752, 200719,   9893,  15268, 120788, 210169,\n",
      "          23224,    261,   1688,   3480,  87431,  13051,      1,      0],\n",
      "        [     0,  28045,    267,  72600,  31773,  32063,  46601,   7715,   3661,\n",
      "           4484,   4066,      1,      0,      0,      0,      0,      0],\n",
      "        [     0,    259,  21410,  76581,  27739,  10722,   2219,   1087,    838,\n",
      "         167042, 177378,   3480, 127653,      1,      0,      0,      0],\n",
      "        [     0,    259, 142017, 148428,   3586, 100289,  96280,  28251,  27378,\n",
      "          21158,   2251,   8922,   4484,  10306,  51385,   4425,   7154]]), 'labels': tensor([[   809,  76581,  10752, 200719,   9893,  15268, 120788, 210169,  23224,\n",
      "            261,   1688,   3480,  87431,  13051,      1,   -100,   -100],\n",
      "        [ 28045,    267,  72600,  31773,  32063,  46601,   7715,   3661,   4484,\n",
      "           4066,      1,   -100,   -100,   -100,   -100,   -100,   -100],\n",
      "        [   259,  21410,  76581,  27739,  10722,   2219,   1087,    838, 167042,\n",
      "         177378,   3480, 127653,      1,   -100,   -100,   -100,   -100],\n",
      "        [   259, 142017, 148428,   3586, 100289,  96280,  28251,  27378,  21158,\n",
      "           2251,   8922,   4484,  10306,  51385,   4425,   7154,      1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouke/Documents/project/stage1/stage1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f3790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f\"loss: {0:>7f}\")\n",
    "    finish_batch_num = (epoch - 1) * len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        progress_bar.set_description(f\"loss: {total_loss / (finish_batch_num + batch):>7f}\")\n",
    "        progress_bar.update(1)\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b42da64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 1.0, 'p': 0.8571428571428571, 'f': 0.9230769181065088}, 'rouge-2': {'r': 0.8, 'p': 0.6666666666666666, 'f': 0.7272727223140496}, 'rouge-l': {'r': 1.0, 'p': 0.8571428571428571, 'f': 0.9230769181065088}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\"\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "scores = rouge.get_scores(\n",
    "    hyps=[generated_summary], refs=[reference_summary]\n",
    ")[0]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a96aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE {'rouge-1': {'r': 0.7142857142857143, 'p': 0.7692307692307693, 'f': 0.7407407357475996}, 'rouge-2': {'r': 0.5714285714285714, 'p': 0.5714285714285714, 'f': 0.5714285664285715}, 'rouge-l': {'r': 0.6428571428571429, 'p': 0.6923076923076923, 'f': 0.6666666616735254}}\n",
      "wrong ROUGE: {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "generated_summary = \"我在苏州大学学计算机，苏州大学很美丽\"\n",
    "reference_summary = \"我在环境优美的苏州大学学计算机\"\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "TOKENIZE_CHINESE = lambda x: ' '.join(x)\n",
    "\n",
    "scores = rouge.get_scores(\n",
    "    hyps=[TOKENIZE_CHINESE(generated_summary)],\n",
    "    refs=[TOKENIZE_CHINESE(reference_summary)]\n",
    ")[0]\n",
    "print(\"ROUGE\", scores)\n",
    "scores = rouge.get_scores(\n",
    "    hyps=[generated_summary],\n",
    "    refs=[reference_summary]\n",
    ")[0]\n",
    "print(\"wrong ROUGE:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32635487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouke/Documents/project/stage1/stage1/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "媒体融合发展是当下中国面临的一大难题。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "checkpoint = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "article_text = \"\"\"\n",
    "受众在哪里，媒体就应该在哪里，媒体的体制、内容、技术就应该向哪里转变。\n",
    "媒体融合关键是以人为本，即满足大众的信息需求，为受众提供更优质的服务。\n",
    "这就要求媒体在融合发展的过程中，既注重技术创新，又注重用户体验。\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    article_text,\n",
    "    return_tensors='pt',\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "generated_tokens = model.generate(\n",
    "    input_ids['input_ids'],\n",
    "    attention_mask=input_ids['attention_mask'],\n",
    "    max_length=512,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4\n",
    ")\n",
    "summary = tokenizer.decode(\n",
    "    generated_tokens[0],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e36ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    preds, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data['input_ids'],\n",
    "                attention_mask=batch_data['attention_mask'],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=2\n",
    "            ).cpu().numpy()\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data['labels'].cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [' '.join(pred.strip()) for pred in decoded_preds]\n",
    "        labels += [' '.join(label.strip()) for label in decoded_labels]\n",
    "    \n",
    "    scores = rouge.get_scores(hyps=preds, refs=labels, avg=True)\n",
    "    result = {key: value['f'] * 100 for key, value in scores.items()}\n",
    "    result['avg'] = np.mean(list(result.values()))\n",
    "    print(f\"Rouge1: {result['rouge-1']:>0.2f} Rouge2: {result['rouge-2']:>0.2f} RougeL: {result['rouge-l']:>0.2f}\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ee1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.590162: 100%|██████████| 500/500 [14:17<00:00,  1.72s/it]\n",
      "100%|██████████| 500/500 [11:46<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 30.82 Rouge2: 18.84 RougeL: 28.09\n",
      "\n",
      "{'rouge-1': 30.822807969169, 'rouge-2': 18.841270642392395, 'rouge-l': 28.08649294438796, 'avg': np.float64(25.91685718531645)}\n",
      "saving new weights...\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "lr = 2e-5\n",
    "epoch_num=1\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num * len(train_dataloader)\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_avg_rouge = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t + 1} / {epoch_num}\\n-------------------------------\")\n",
    "    total_loss += train_loop(train_dataloader, model, optimizer, lr_scheduler, t + 1, total_loss)\n",
    "    valid_rouge = test_loop(valid_dataloader, model)\n",
    "    print(valid_rouge)\n",
    "    rouge_avg = valid_rouge['avg']\n",
    "    if rouge_avg > best_avg_rouge:\n",
    "        best_avg_rouge = rouge_avg\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f\"epoch_{t+1}_valid_rouge_{rouge_avg:0.4f}_model_weights.bin\")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a98e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
